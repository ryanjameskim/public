{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Constellation - ETF2Vec Batch Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMCDlRJZoiPbpi8Lex9k/Gq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanjameskim/public/blob/master/210427_ETF2Vec_Batch_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y374x4g7hbUL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Reshape, Embedding, Dense, Dot, Flatten\n",
        "from tensorflow.keras import Model\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSBGvAKNhlnq",
        "outputId": "f209cc84-dc83-4da2-8a38-c7e57efb7183"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWIevksthn21"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/ML/Ishares Data/210331/Holdings_small/Aggregate.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nM0erOWh5oZ"
      },
      "source": [
        "df = df.sort_values(['ETF', 'Weight (%)'], ascending=[True, False])\n",
        "\n",
        "#Find tickers and frequencies\n",
        "val_cnts = df['Ticker'].value_counts() \n",
        "etf_names = df['ETF'].value_counts().index.to_numpy()\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(etf_names)\n",
        "\n",
        "#Create vocabulary and integerization of stock tickers\n",
        "vocab_size = len(val_cnts)  #3866\n",
        "idx_to_tick = {i + 1: ticker for i, ticker in enumerate(val_cnts.keys())}  #natural counting\n",
        "idx_to_tick[0] = '<PAD>'\n",
        "tick_to_idx = {ticker : index for index, ticker in idx_to_tick.items()}   #inverse\n",
        "\n",
        "#constants\n",
        "window_size = 3    # three nearest in portfolio % weight to the stock in question\n",
        "vector_dim = 128    # dimensionality of the embedding vector\n",
        "epochs = 10 ** 6    # one million epochs\n",
        "num_ns = 10  # number of negative samples\n",
        "SEED = 42 # for replication\n",
        "AUTOTUNE = tf.data.AUTOTUNE # for caching\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgBRTgKRiAM4",
        "outputId": "65d61010-d79f-48d1-e679-6ee76d955f99"
      },
      "source": [
        "#***SINGLE EXAMPLE***#\n",
        "#example\n",
        "example_etf = [tick_to_idx[tick] for tick in df[df['ETF'] == etf_names[90]]['Ticker']]\n",
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_etf,\n",
        "      vocabulary_size=vocab_size + 1,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(example_etf))  # 125\n",
        "print(len(positive_skip_grams))   #494 = 125 * 4 - 6\n",
        "\n",
        "#look at example\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "    print(f'{target}, {context}: {idx_to_tick[target]}, {idx_to_tick[context]}')\n",
        "\n",
        "#negative sampling example\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=42,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "#!!!Because this returns [0, vocab_size), must add +1 to get proper idx index\n",
        "\n",
        "print(negative_sampling_candidates) #Note the zero: tf.Tensor([ 77   0 143 265 103], shape=(5,), dtype=int64)\n",
        "print([idx_to_tick[idx.numpy() + 1] for idx in negative_sampling_candidates])\n",
        "\n",
        "#manually create one training example\n",
        "#expand dimensions\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "#concatenate with positive example\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label = tf.squeeze(label)\n",
        "\n",
        "\n",
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {idx_to_tick[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[idx_to_tick[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125\n",
            "494\n",
            "578, 36: ZM, ZTS\n",
            "164, 36: GRMN, ZTS\n",
            "33, 225: BIIB, NVR\n",
            "15, 58: EA, CDNS\n",
            "578, 51: ZM, TSCO\n",
            "tf.Tensor([164  18 788  55 297 504 131   0 176 826], shape=(10,), dtype=int64)\n",
            "['PHM', 'DVA', 'NVCR', 'HPQ', 'XOM', 'Z', 'SCHW', 'PSA', 'HST', 'SWX']\n",
            "target_index    : 578\n",
            "target_word     : ZM\n",
            "context_indices : [ 36 164  18 788  55 297 504 131   0 176 826]\n",
            "context_words   : ['ZTS', 'GRMN', 'INTC', 'CC', 'AVGO', 'MPC', 'RGLD', 'STE', '<PAD>', 'NEM', 'USM']\n",
            "label           : [1 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5owlCSwkiCH_",
        "outputId": "9af99eec-7f96-4dca-a770-c109a838e22b"
      },
      "source": [
        "#function to create skipgrams per ETF\n",
        "def generate_training_data(etf_names, window_size, num_ns, vocab_size, seed):\n",
        "    # Elements of each training example are appended to these lists.\n",
        "    targets, contexts, labels = [], [], []\n",
        "    # Build the sampling table for vocab_size tokens.\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\n",
        "                        vocab_size + 1) #+1 due to index zero being skipped\n",
        "    #for each ETF, generate positive skip-gram\n",
        "    for etf in etf_names:\n",
        "        tokens = np.array([tick_to_idx[tick] for tick in df.loc[df['ETF'] == etf,\n",
        "                                                            'Ticker'].values]) \n",
        "        positive_skip_grams, _ = skipgrams(\n",
        "                            tokens,\n",
        "                            vocab_size + 1,\n",
        "                            window_size = window_size,\n",
        "                            negative_samples = 0)\n",
        "        #Iterate over each positive skip-gram for negative samples\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_class = tf.expand_dims(\n",
        "                                tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "                                                true_classes=context_class,\n",
        "                                                num_true=1,\n",
        "                                                num_sampled=num_ns,\n",
        "                                                unique=True,\n",
        "                                                range_max=vocab_size,\n",
        "                                                seed=SEED,\n",
        "                                                name=\"negative_sampling\")\n",
        "            negative_sampling_candidates += 1\n",
        "            # Build context and label vectors (for one target word)\n",
        "            negative_sampling_candidates = tf.expand_dims(\n",
        "                                            negative_sampling_candidates, 1)\n",
        "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "            targets.append(target_word)\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "    return targets, contexts, labels\n",
        "\n",
        "targets, contexts, labels = generate_training_data(\n",
        "    etf_names=etf_names,\n",
        "    window_size=window_size,\n",
        "    num_ns=num_ns,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))\n",
        "\n",
        "#make dataset using tf.data.Dataset API\n",
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "#dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "248586 248586 248586\n",
            "<BatchDataset shapes: (((1024,), (1024, 11, 1)), (1024, 11)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTYz92G2iFQk",
        "outputId": "2f7af905-6d20-4d18-8c50-b13394a93805"
      },
      "source": [
        "\n",
        "#***#Keras Subclassing API Model***#\n",
        "class ETF2Vec(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(ETF2Vec, self).__init__()\n",
        "        self.target_embedding = Embedding(vocab_size + 1,\n",
        "                                          vector_dim,\n",
        "                                          input_length=1,\n",
        "                                          name=\"E2V_embedding\")\n",
        "        self.context_embedding = Embedding(vocab_size + 1,\n",
        "                                           vector_dim,\n",
        "                                           input_length=num_ns+1)\n",
        "        self.dots = Dot(axes=(3, 2))\n",
        "        self.flatten = Flatten()\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        word_emb = self.target_embedding(target)\n",
        "        context_emb = self.context_embedding(context)\n",
        "        dots = self.dots([context_emb, word_emb])\n",
        "        return self.flatten(dots)\n",
        "\n",
        "\n",
        "#**Compile Model**#\n",
        "etf2vec = ETF2Vec(vocab_size, vector_dim)\n",
        "etf2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#Callback for Tensorboard\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "\n",
        "etf2vec.fit(dataset, epochs=100, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "242/242 [==============================] - 5s 13ms/step - loss: 2.3385 - accuracy: 0.2224\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 1.6933 - accuracy: 0.3941\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 1.2511 - accuracy: 0.6129\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 0.9995 - accuracy: 0.6961\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 0.8634 - accuracy: 0.7402\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 0.7725 - accuracy: 0.7650\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 0.7028 - accuracy: 0.7835\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 0.6472 - accuracy: 0.7997\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 3s 12ms/step - loss: 0.5986 - accuracy: 0.8155\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.5550 - accuracy: 0.8301\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.5188 - accuracy: 0.8434\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.4846 - accuracy: 0.8550\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.4524 - accuracy: 0.8670\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.4235 - accuracy: 0.8772\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.3976 - accuracy: 0.8855\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.3741 - accuracy: 0.8932\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.3523 - accuracy: 0.9006\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.3328 - accuracy: 0.9065\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.3143 - accuracy: 0.9123\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2966 - accuracy: 0.9169\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2813 - accuracy: 0.9211\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2679 - accuracy: 0.9254\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2552 - accuracy: 0.9295\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2420 - accuracy: 0.9336\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2309 - accuracy: 0.9371\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2216 - accuracy: 0.9394\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2115 - accuracy: 0.9423\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.2018 - accuracy: 0.9457\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1938 - accuracy: 0.9481\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1864 - accuracy: 0.9503\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1788 - accuracy: 0.9524\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1710 - accuracy: 0.9546\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1652 - accuracy: 0.9562\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1593 - accuracy: 0.9580\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1537 - accuracy: 0.9598\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1482 - accuracy: 0.9615\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1430 - accuracy: 0.9633\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1383 - accuracy: 0.9644\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1340 - accuracy: 0.9657\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1298 - accuracy: 0.9665\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1255 - accuracy: 0.9679\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1221 - accuracy: 0.9685\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1185 - accuracy: 0.9692\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1157 - accuracy: 0.9698\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1118 - accuracy: 0.9709\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1099 - accuracy: 0.9714\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1069 - accuracy: 0.9720\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1040 - accuracy: 0.9726\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.1015 - accuracy: 0.9732\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0993 - accuracy: 0.9736\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0971 - accuracy: 0.9740\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0950 - accuracy: 0.9747\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0932 - accuracy: 0.9750\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0915 - accuracy: 0.9753\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0901 - accuracy: 0.9755\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0878 - accuracy: 0.9762\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0865 - accuracy: 0.9763\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0848 - accuracy: 0.9765\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0835 - accuracy: 0.9770\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0821 - accuracy: 0.9772\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0814 - accuracy: 0.9774\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0801 - accuracy: 0.9777\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0790 - accuracy: 0.9778\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0775 - accuracy: 0.9782\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0767 - accuracy: 0.9782\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0762 - accuracy: 0.9781\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0752 - accuracy: 0.9785\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0737 - accuracy: 0.9788\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0729 - accuracy: 0.9788\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0727 - accuracy: 0.9786\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0717 - accuracy: 0.9789\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0713 - accuracy: 0.9789\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0707 - accuracy: 0.9789\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0695 - accuracy: 0.9791\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0694 - accuracy: 0.9790\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0685 - accuracy: 0.9791\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0677 - accuracy: 0.9792\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0673 - accuracy: 0.9793\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0670 - accuracy: 0.9793\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0665 - accuracy: 0.9792\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0653 - accuracy: 0.9794\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0652 - accuracy: 0.9796\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0649 - accuracy: 0.9794\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0645 - accuracy: 0.9796\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0642 - accuracy: 0.9795\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0635 - accuracy: 0.9798\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0633 - accuracy: 0.9797\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0627 - accuracy: 0.9798\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0623 - accuracy: 0.9797\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0619 - accuracy: 0.9798\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0621 - accuracy: 0.9796\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0613 - accuracy: 0.9798\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0612 - accuracy: 0.9799\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0608 - accuracy: 0.9797\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0609 - accuracy: 0.9800\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0603 - accuracy: 0.9799\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0603 - accuracy: 0.9800\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0601 - accuracy: 0.9797\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0597 - accuracy: 0.9798\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 3s 11ms/step - loss: 0.0595 - accuracy: 0.9798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff827d51d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Qc_CA5fYiSMM",
        "outputId": "2948ef2f-1a6e-4684-848f-9c182a9d4edf"
      },
      "source": [
        "\n",
        "#For saving vectors and metadata\n",
        "weights = etf2vec.get_layer('E2V_embedding').get_weights()[0]\n",
        "vocab = ['<PAD>'] + val_cnts.keys().to_list()\n",
        "\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "#For use in Google Colabs\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9a6b3410-d5e0-4031-bad5-9c5b755bf3a1\", \"vectors.tsv\", 5514398)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_eb92cd01-2503-466d-936b-bf61f3cad7ee\", \"metadata.tsv\", 17789)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArNSgnI_i6hB"
      },
      "source": [
        "#validation constants\n",
        "\n",
        "custom_examples = ['CSCO', 'NKE', 'INTC', 'GS', 'T', 'TSLA', 'AAPL', 'PAYX']\n",
        "valid_examples = [tick_to_idx[tick] for tick in custom_examples]\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZWAM0YGo7pZ",
        "outputId": "85527db5-2639-4288-8ec7-7ce89517b172"
      },
      "source": [
        "#simple cosine similarity with resultant weights\n",
        "\n",
        "def get_sim(valid_ex_id):\n",
        "  sim = np.zeros((vocab_size,))  #initalize sim and inputs\n",
        "  for i in range(vocab_size):     #for each ticker, test the validation ticker against each other stock \n",
        "      sim[i] = np.sum(weights[valid_ex_id] * weights[i])\n",
        "  return sim\n",
        "\n",
        "for i, valid_ex_id in enumerate(valid_examples):       #goes through each of the validation examples\n",
        "  valid_name = idx_to_tick[valid_ex_id]       #find name of validation ticker \n",
        "  top_k = 8                                # number of nearest neighbors\n",
        "  sim = get_sim(valid_ex_id)        #get similarity against all other tickers\n",
        "  nearest = (-sim).argsort()[1:top_k + 1]  #gets negative of the cosine scores, lists the indexes of the most negative to positve scores\n",
        "                                            #then indexing the argsort lists the smallest numbers, skips index 0 because it will be -1 (the cosine similarity to itself) \n",
        "  log_str = f'Nearest to {valid_name}:'\n",
        "  for kth_near in nearest:\n",
        "      close_tick = idx_to_tick[kth_near]\n",
        "      log_str = f'{log_str} {close_tick},'  #keeps appending to the log string\n",
        "  print(log_str)\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nearest to CSCO: XOM, GOOGL, KO, AMED, MO, CMCSA, CRM, CVX,\n",
            "Nearest to NKE: HON, AAPL, WPC, ACN, ABBV, AMD, QRTEA, HLF,\n",
            "Nearest to INTC: MA, NVDA, NFLX, AAPL, JNJ, MSFT, CRM, V,\n",
            "Nearest to GS: DE, ETP, CFX, LMT, OUT, RWT, BLK, WPC,\n",
            "Nearest to T: XOM, FTV, TEAM, JPM, RHP, FLO, NFG, AVGO,\n",
            "Nearest to TSLA: FB, AMZN, GOOGL, AAPL, MSFT, WHR, UNH, INDA,\n",
            "Nearest to AAPL: ACI, GOOGL, MSFT, AMZN, LSCC, FB, FAF, GOOG,\n",
            "Nearest to PAYX: AZO, MTCH, AXTA, WCN, LSI, GE, DAL, TDG,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfTu3a9CrGzt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
